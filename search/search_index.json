{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ML-Notes","text":""},{"location":"cv/about/","title":"About","text":"<p>Hi!</p>"},{"location":"home/about/","title":"About","text":"<p>Hi!</p>"},{"location":"linear-algebra/about/","title":"Linear Algebra/table","text":""},{"location":"linear-algebra/about/#books","title":"Books","text":"<ol> <li>\"Linear Algebra Done Right\" by Sheldon Axler</li> <li>\"Linear Algebra and Its Applications\" by David C. Lay, Steven R. Lay, and Judi J. McDonald</li> <li>\u201cIntroduction to Linear Algebra\u201d by Gilbert Strang</li> </ol>"},{"location":"linear-algebra/about/#resources","title":"Resources","text":"Name URL Bhagwan Singh Playlist (LA) https://www.youtube.com/playlist?list=PLdM-WZokR4taLvoJPvfHwF8m0Q1K6Qvmz MKS Playlist (Matrices) https://youtube.com/playlist?list=PLhSp9OSVmeyIVQpCt2kwsC1dNVl1GwlVn&amp;si=N4hQBEKy0jhuO2u4 Identities https://dustinstansbury.github.io/theclevermachine/linear-algebra-identities Matrix Properties https://math.mit.edu/~dyatlov/54summer10/matalg.pdf"},{"location":"linear-algebra/about/#chapters","title":"Chapters","text":"<p>Introduction to Vectors</p> <p>Solving Linear Equations</p> <p>Vector Spaces and Subspaces</p> <p>Orthogonality</p> <p>Determinants</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/","title":"Lengths and Dot Products","text":""},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#lengthmagnitude","title":"Length/Magnitude","text":"<p>An important case of the dot product is when a vector is dotted with itself. In this case, we have \\(v = w\\).</p> <p>For example, if \\(v=(1,2,3)\\), then the dot product with itself is: \\(\\mathbf{v} \\cdot \\mathbf{v} = \\|\\mathbf{v}\\|^2 = 1^2 + 2^2 + 3^2 = 14\\)</p> <p>This value is known as the dot product \\(\\mathbf{v} \\cdot \\mathbf{v}\\), which also equals the squared length of the vector. There is no angle between a vector and itself; we can think of this angle as \\(0^\\circ\\), not \\(90^\\circ\\). The result is not zero because a vector is never perpendicular to itself.</p> <p>The dot product \\(\\mathbf{v} \\cdot \\mathbf{v}\\) gives the length squared of the vector.</p> <p>Definition: The length (or norm) \\(\\|\\mathbf{v}\\|\\) of a vector \\(\\mathbf{v}\\) is the square root of the dot product of the vector with itself:</p> \\[ length = \\|\\mathbf{v}\\| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}} = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} \\] <p>where \\(n\\) is the number of dimensions.</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#unit-vector","title":"Unit Vector","text":"<p>The word \"unit\" always indicates that some measurement equals one. For example:</p> <ul> <li>A unit price is the price for one item.</li> <li>A unit cube has sides of length one.</li> <li>A unit circle is a circle with radius one.</li> <li></li> </ul> <p>Definition: A unit vector \\(u\\) is a vector whose length equals one. This implies: \\(u\u22c5u=1\\)</p> <p>An example in four dimensions is: \\(\\mathbf{u} = \\left( \\tfrac{1}{2}, \\tfrac{1}{2}, \\tfrac{1}{2}, \\tfrac{1}{2} \\right).\\)</p> <p>Then the dot product is: \\(\\mathbf{u} \\cdot \\mathbf{u} = \\left( \\tfrac{1}{2} \\right)^2 + \\left( \\tfrac{1}{2} \\right)^2 + \\left( \\tfrac{1}{2} \\right)^2 + \\left( \\tfrac{1}{2} \\right)^2 = \\tfrac{1}{4} + \\tfrac{1}{4} + \\tfrac{1}{4} + \\tfrac{1}{4} = 1\\).</p> <p>We obtained this unit vector by dividing the vector \\(\\mathbf{v} = (1, 1, 1, 1)\\) by its length: \\(\\|\\mathbf{v}\\| = \\sqrt{1^2 + 1^2 + 1^2 + 1^2} = \\sqrt{4} = 2\\)</p> <p>so the unit vector is: \\(\\mathbf{u} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|} = \\left( \\tfrac{1}{2}, \\tfrac{1}{2}, \\tfrac{1}{2}, \\tfrac{1}{2} \\right)\\).</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#inequalities","title":"Inequalities","text":"<p>No matter the angle between the vectors, the dot product of \\(\\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\\) with \\(\\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|}\\) never exceeds one.</p> <p>This fact is captured by the Cauchy\u2013Schwarz inequality, also known historically as the Schwarz inequality or the Cauchy\u2013Schwarz\u2013Bunyakovsky inequality. It was discovered independently in France, Germany, and Russia. This inequality is one of the most important in all of mathematics.</p> <p>Since \\(\\cos \\theta| \\leq 1\\), the cosine formula for the dot product implies two foundational inequalities:</p> <p>Schwarz Inequality: \\(|\\mathbf{v} \\cdot \\mathbf{w}| \\leq \\|\\mathbf{v}\\| \\, \\|\\mathbf{w}\\|\\)</p> <p>Triangle Inequality: \\(\\|\\mathbf{v} + \\mathbf{w}\\| \\leq \\|\\mathbf{v}\\| + \\|\\mathbf{w}\\|\\)</p> <p>These inequalities hold for any vectors \\(v\\) and \\(w\\) in Euclidean space. These inequalities are essential in proving orthogonality, estimating angles, and ensuring numerical stability in computations.</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#dot-product","title":"Dot Product","text":"<p>The dot product (also called scalar product) takes two vectors and gives a number (a scalar), not a vector.</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#formula","title":"Formula","text":"<p>If</p> \\[ \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\] <p>then:The dot product w \u00b7 v equals v \u00b7 w. The order of v and w makes no difference.</p> \\[ \\mathbf{v} \\cdot \\mathbf{w} = v_1 w_1 + v_2 w_2 \\] <p>In 3D:</p> \\[ \\mathbf{v} \\cdot \\mathbf{w} = v_1 w_1 + v_2 w_2 + v_3 w_3 \\]  \ud83d\udca1  The dot product $w \\cdot v$ equals $v \\cdot w$. The order of v and w makes no difference."},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#geometric-meaning","title":"Geometric Meaning","text":"<p>The dot product also equals:</p> \\[ \\mathbf{v} \\cdot \\mathbf{w} = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\cos(\\theta) \\] <p>where \u03b8 is the angle between the vectors.</p> <p>This tells us:</p> <ul> <li>If the dot product is positive, the angle is acute (&lt; 90\u00b0) (~same direction)</li> <li>If it is zero, the vectors are perpendicular (orthogonal)</li> <li>If it is negative, the angle is obtuse (&gt; 90\u00b0) (~opposite direction)</li> </ul>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#example","title":"Example","text":"<p>Let</p> \\[ \\mathbf{v} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} 4 \\\\ -2 \\end{bmatrix} \\] <p>Then:</p> \\[ \\mathbf{v} \\cdot \\mathbf{w} = 1 \\cdot 4 + 3 \\cdot (-2) = 4 - 6 = -2 \\] <p>The result is a number: -2.</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#working","title":"Working","text":"<ul> <li>The dot product helps project one vector onto another.</li> <li>It measures how much one vector extends in the direction of another.</li> <li>Used to find angles between vectors and to project one vector onto another.</li> <li>In physics, work = force \u22c5 displacement.</li> </ul>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#cross-product","title":"Cross Product","text":"<p>The cross product is only defined in 3D. It takes two vectors and gives a new vector, not a number.</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#formula_1","title":"Formula","text":"<p>If</p> \\[ \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\end{bmatrix} \\] <p>then:</p> \\[ \\mathbf{v} \\times \\mathbf{w} = \\begin{vmatrix} \\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k} \\\\ v_1 &amp; v_2 &amp; v_3 \\\\ w_1 &amp; w_2 &amp; w_3 \\end{vmatrix} \\\\ \\mathbf{v} \\times \\mathbf{w} = (v_2 w_3 - v_3 w_2)\\mathbf{i} - (v_3 w_1 - v_1 w_3)\\mathbf{j} + (v_1 w_2 - v_2 w_1)\\mathbf{k} \\\\ \\mathbf{v} \\times \\mathbf{w} = \\begin{bmatrix}  v_2 w_3 - v_3 w_2 \\\\ v_3 w_1 - v_1 w_3 \\\\ v_1 w_2 - v_2 w_1 \\end{bmatrix} \\] <p>This new vector is perpendicular to both v and w.</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#geometric-meaning_1","title":"Geometric Meaning","text":"<p>The length of the cross product is:</p> \\[ ||\\mathbf{v} \\times \\mathbf{w}\\| = \\|\\mathbf{v}\\| \\|\\mathbf{w}\\| \\sin(\\theta) \\] <p>This is the area of the parallelogram formed by the two vectors.</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#right-hand-rule","title":"Right-Hand Rule","text":"<p>To find the direction of the cross product:</p> <ol> <li>Point your right-hand fingers along v</li> <li>Curl toward w</li> <li>Your thumb points in the direction of \\(\\mathbf{v} \\times \\mathbf{w}\\)</li> </ol>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#example_1","title":"Example","text":"<p>Let</p> \\[ \\mathbf{v} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\] <p>Then:</p> \\[ \\mathbf{v} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\\\ \\mathbf{v} \\times \\mathbf{w} = \\begin{vmatrix} \\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k} \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{vmatrix} \\\\ \\mathbf{v} \\times \\mathbf{w} = \\mathbf{i}(0 \\cdot 0 - 0 \\cdot 1) - \\mathbf{j}(1 \\cdot 0 - 0 \\cdot 0) + \\mathbf{k}(1 \\cdot 1 - 0 \\cdot 0) \\mathbf{v} \\times \\mathbf{w} = \\mathbf{i}(0) - \\mathbf{j}(0) + \\mathbf{k}(1) \\\\ \\mathbf{v} \\times \\mathbf{w} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\] <p>The result is a vector pointing along the z-axis.</p>"},{"location":"linear-algebra/introduction-to-vectors/lengths-and-dot-products/#working_1","title":"Working","text":"<ul> <li>Produces a vector perpendicular (normal) to both input vectors.</li> <li>Direction follows the right-hand rule.</li> <li>Magnitude equals the area of the parallelogram formed by the two vectors.</li> <li>Zero vector if the vectors are parallel or one is zero.</li> <li>Useful for finding normals to planes and calculating torque.</li> </ul>"},{"location":"linear-algebra/introduction-to-vectors/matrices/","title":"Matrices","text":""},{"location":"linear-algebra/introduction-to-vectors/matrices/#properties","title":"Properties","text":"<p>Properties of matrix operations:</p> <ul> <li>Addition: If \\(A\\) and \\(B\\) are matrices of the same size \\(m\u00d7n\\), then their sum \\(A+B\\) is also an \\(m\u00d7n\\) matrix.</li> <li>Multiplication by scalars: If \\(A\\) is an \\(m\u00d7n\\) matrix and \\(c\\) is a scalar, then \\(cA\\) is an \\(m\u00d7n\\) matrix.</li> <li>Matrix multiplication: If \\(A\\) is an \\(m\u00d7n\\) matrix and \\(B\\) is an \\(n\u00d7p\\) matrix, then the product \\(AB\\) is an \\(m\u00d7p\\) matrix.</li> <li>Vectors: A vector of length \\(n\\) can be treated as an \\(n\u00d71\\) matrix. Vector addition, scalar multiplication, and matrix-vector multiplication follow the same rules as matrix operations.</li> <li>Transpose: For an \\(m\u00d7n\\) matrix \\(A\\), its transpose \\(A^T\\) is an \\(n\u00d7m\\) matrix.</li> <li>Identity matrix: \\(I_n\\) is the \\(n\u00d7n\\) identity matrix, with 1's on the diagonal and 0's elsewhere.</li> <li>Zero matrix: Denoted by 0, it is a matrix of all zeroes with appropriate size.</li> <li>Inverse: For a square matrix \\(A\\), its inverse \\(A^{-1}\\) is a matrix of the same size such that \\(AA^{-1} = A^{-1}A = I_n\\). Not all matrices have inverses; those that do are called invertible.</li> </ul> <p>Key properties (assuming scalars \\(r, s\\) and appropriately sized matrices \\(A, B, C\\)):</p> <p>Properties of matrix addition:</p> \\[ \\begin{aligned} &amp; A + B = B + A &amp;&amp; \\text{(Commutativity)} \\\\ &amp; (A + B) + C = A + (B + C) &amp;&amp; \\text{(Associativity)} \\\\ &amp; A + 0 = A, &amp;&amp; \\text{(Additive identity)} \\\\ &amp; r(A + B) = rA + rB &amp;&amp; \\text{(Distributivity of scalar over addition)} \\\\ &amp; (r + s)A = rA + sA &amp;&amp; \\text{(Distributivity of addition over scalar)} \\\\ &amp; r(sA) = (rs)A &amp;&amp; \\text{(Associativity of scalar multiplication)} \\\\ \\end{aligned} \\] <p>Properties of matrix multiplication:</p> \\[ \\begin{aligned} &amp; A(BC) = (AB)C &amp;&amp; \\text{(Associativity)} \\\\ &amp; A(B + C) = AB + AC &amp;&amp; \\text{(Left distributivity)} \\\\ &amp; (B + C)A = BA + CA &amp;&amp; \\text{(Right distributivity)} \\\\ &amp; r(AB) = (rA)B = A(rB) &amp;&amp; \\text{(Scalar multiplication compatibility)} \\\\ &amp; I_m A = A = A I_n &amp;&amp; \\text{(Identity matrix)} \\\\ \\end{aligned} \\] <p>Properties of the transpose operation:</p> \\[ \\begin{aligned} &amp; (A^T)^T = A, &amp;&amp; \\text{(Double transpose)} \\\\ &amp; (A + B)^T = A^T + B^T &amp;&amp; \\text{(Transpose of sum)} \\\\ &amp; (rA)^T = r A^T &amp;&amp; \\text{(Transpose of scalar multiple)} \\\\ &amp; (AB)^T = B^T A^T &amp;&amp; \\text{(Transpose of product reverses order)} \\\\ &amp; (I_n)^T = I_n &amp;&amp; \\text{(Transpose of identity)} \\\\ \\end{aligned} \\] <p>Properties of the inverse operation (for invertible matrices):</p> \\[ \\begin{aligned} &amp; A A^{-1} = A^{-1} A = I_n, &amp;&amp; \\text{(Inverse definition)} \\\\ &amp; (rA)^{-1} = r^{-1} A^{-1} \\quad r \\neq 0, &amp;&amp; \\text{(Inverse of scalar multiple)} \\\\ &amp; (AB)^{-1} = B^{-1} A^{-1} &amp;&amp; \\text{(Inverse of product reverses order)} \\\\ &amp; (I_n)^{-1} = I_n &amp;&amp; \\text{(Inverse of identity)} \\\\ &amp; (A^T)^{-1} = (A^{-1})^T &amp;&amp; \\text{(Transpose and inverse commute)} \\\\ &amp; (A^{-1})^{-1} = A &amp;&amp; \\text{(Inverse of inverse)} \\\\ \\end{aligned} \\] <p>Additional Properties</p> \\[ \\begin{aligned} &amp; A = A^T \\quad \\text{(Symmetric matrix)} \\\\ &amp; Q^T Q = I_n \\quad \\text{(Orthogonal matrix)} \\\\ &amp; \\mathrm{tr}(A + B) = \\mathrm{tr}(A) + \\mathrm{tr}(B) \\quad \\text{(Trace linearity)} \\\\ &amp; \\mathrm{tr}(AB) = \\mathrm{tr}(BA) \\quad \\text{(Trace cyclic property)} \\\\ &amp; \\det(AB) = \\det(A) \\det(B) \\quad \\text{(Determinant multiplicative)} \\\\ &amp; \\det(A^T) = \\det(A) \\quad \\text{(Determinant transpose)} \\\\ &amp; \\det(A^{-1}) = \\frac{1}{\\det(A)} \\quad \\text{(Determinant inverse)} \\\\ &amp; \\operatorname{rank}(A) \\leq \\min(m, n) \\quad \\text{for } A \\in \\mathbb{R}^{m \\times n} \\\\ &amp; \\operatorname{rank}(AB) \\leq \\min(\\operatorname{rank}(A), \\operatorname{rank}(B)) \\\\ &amp; \\text{If } A \\text{ is invertible and } AB = AC \\text{ then } B = C \\quad \\text{(Cancellation law)} \\\\ &amp; \\text{In general, } AB \\neq BA \\quad \\text{(Non-commutativity)} \\\\ &amp; (A + B)(A - B) = A^2 - B^2 \\quad \\text{only if } AB = BA \\quad \\text{(Difference of squares)} \\\\ &amp; \\text{If } A \\text{ is symmetric and positive definite, } \\exists \\text{ Cholesky factorization } A = LL^T \\\\ &amp; \\text{Eigenvalues of } A^T = \\text{Eigenvalues of } A \\\\ &amp; \\text{Eigenvalues of } A^{-1} = \\frac{1}{\\text{Eigenvalues of } A} \\end{aligned} \\] <p>Differences from regular number operations:</p> <ul> <li>Matrix multiplication is generally not commutative; in general, \\(AB \\neq BA\\).</li> <li>The transpose of a product reverses order: \\((AB)^T = B^T A^T\\).</li> <li>The inverse of a product reverses order: \\((AB)^{-1} = B^{-1} A^{-1}\\).</li> <li>To conclude \\(B = C\\) from \\(AB = AC\\), matrix \\(A\\) must be invertible.</li> <li>If \\(AB = 0\\), it does not imply \\(A = 0\\) or \\(B = 0\\). For example,</li> </ul> \\[ A = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 0 &amp; 0 \\\\ 0 &amp; 1 \\end{pmatrix}, \\quad AB = \\begin{pmatrix} 0 &amp; 0 \\\\ 0 &amp; 0 \\end{pmatrix} \\]  \ud83d\udca1  A square matrix is invertible if and only if its determinant is non-zero."},{"location":"linear-algebra/introduction-to-vectors/matrices/#types","title":"Types","text":"<p>Matrices come in many forms depending on their size, elements, and special properties. Below is a detailed overview of the most common types of matrices used in linear algebra.</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#1-square-matrix","title":"1. Square Matrix","text":"<p>A matrix with the same number of rows and columns.</p> \\[ A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nn} \\end{bmatrix} \\] <p>Example: \\(\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{bmatrix}\\)</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#2-rectangular-matrix","title":"2. Rectangular Matrix","text":"<p>A matrix where the number of rows is not equal to the number of columns.</p> \\[ B = \\begin{bmatrix} b_{11} &amp; b_{12} &amp; \\cdots &amp; b_{1p} \\\\ b_{21} &amp; b_{22} &amp; \\cdots &amp; b_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ b_{m1} &amp; b_{m2} &amp; \\cdots &amp; b_{mp} \\end{bmatrix} \\] <p>with \\(m\\neq n\\) .</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#3-row-matrix","title":"3. Row Matrix","text":"<p>A matrix with only one row \\(1 \\times n\\).</p> \\[ R = \\begin{bmatrix} r_1 &amp; r_2 &amp; \\cdots &amp; r_n \\end{bmatrix} \\]"},{"location":"linear-algebra/introduction-to-vectors/matrices/#4-column-matrix","title":"4. Column Matrix","text":"<p>A matrix with only one column \\(m \\times 1\\).</p> \\[ C = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_m \\end{bmatrix} \\]"},{"location":"linear-algebra/introduction-to-vectors/matrices/#5-zero-matrix-null-matrix","title":"5. Zero Matrix (Null Matrix)","text":"<p>A matrix in which all elements are zero.</p> \\[ O = \\begin{bmatrix} 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 \\end{bmatrix}\u00a0 \\]"},{"location":"linear-algebra/introduction-to-vectors/matrices/#6-identity-matrix","title":"6. Identity Matrix","text":"<p>A square matrix with ones on the main diagonal and zeros elsewhere.</p> \\[ I_n = \\begin{bmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{bmatrix} \\]"},{"location":"linear-algebra/introduction-to-vectors/matrices/#7-diagonal-matrix","title":"7. Diagonal Matrix","text":"<p>A square matrix where all off-diagonal elements are zero.</p> \\[ D = \\begin{bmatrix} d_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; d_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; d_n \\end{bmatrix} \\]"},{"location":"linear-algebra/introduction-to-vectors/matrices/#8-scalar-matrix","title":"8. Scalar Matrix","text":"<p>A diagonal matrix where all diagonal entries are equal.</p> \\[ S = \\lambda I_n = \\begin{bmatrix} \\lambda &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\lambda &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\lambda \\end{bmatrix} \\]"},{"location":"linear-algebra/introduction-to-vectors/matrices/#9-symmetric-matrix","title":"9. Symmetric Matrix","text":"<p>A square matrix that is equal to its transpose: \\(A = A^T\\)</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#10-skew-symmetric-antisymmetric-matrix","title":"10. Skew-Symmetric (Antisymmetric) Matrix","text":"<p>A square matrix whose transpose equals its negative: \\(A^T = -A\\)</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#11-orthogonal-matrix","title":"11. Orthogonal Matrix","text":"<p>A square matrix \\(Q\\) with real entries whose transpose is its inverse: </p> \\[ Q^T = Q^{-1} \\\\\\text{OR}\\\\ Q^T Q = Q Q^T = I_n \\] <p>Example: A  rotation matrix \\(Q = \\begin{bmatrix} \\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{bmatrix}\\)</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#12-singular-matrix","title":"12. Singular Matrix","text":"<p>A square matrix that does not have an inverse. Its determinant is zero: \\(\\det(A) = 0\\)</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#13-invertible-nonsingular-matrix","title":"13. Invertible (Nonsingular) Matrix","text":"<p>A square matrix \\(A\\) that has an inverse: \\(A^{-1} = A^{-1} A = I_n\\)</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#14-positive-definite-matrix","title":"14. Positive Definite Matrix","text":"<p>A symmetric matrix \\(A\\) such that for all nonzero vectors \\(x\\), \\(x^T A x &gt; 0\\)</p> <p>Its eigenvalues are strictly positive. If the quadratic form is strictly greater than zero for all non-zero vectors x, then the matrix is called positive definite.</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#15-positive-semi-definite-psd-matrix","title":"15. Positive Semi-Definite (PSD) Matrix","text":"<p>A symmetric matrix \\(A\\) is positive semi-definite if for all vectors \\(x\\), \\(x^T A x \\geq 0\\)</p> <p>Its eigenvalues are non-negative. This means the quadratic form is never negative, but it can be zero for some nonzero \\(x\\).</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#16-hermitian-matrix","title":"16. Hermitian Matrix","text":"<p>A square matrix \\(A\\) with complex entries is Hermitian if it equals its own conjugate transpose: \\(A = A^H = \\overline{A}^T\\)</p> <p>This means \\(A_{ij} = \\overline{A_{ji}}\\) for all \\(i,j\\).</p> <p>Example: \\(A = \\begin{bmatrix} 2 &amp; 2 + i \\\\ 2 - i &amp; 3 \\end{bmatrix}\\)Here, \\(A = A^H\\).</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#17-skew-hermitian-matrix","title":"17. Skew-Hermitian Matrix","text":"<p>A square matrix A is skew-Hermitian if it satisfies: \\(A = -A^H = -\\overline{A}^T\\)(i.e. if and only if it is equal to the negative of its conjugate matrix).</p> <p>This means \\(A_{ij} = -\\overline{A_{ji}}\\).</p> <p>Example: \\(A = \\begin{bmatrix} 0 &amp; 2 + i \\\\ -2 + i &amp; 0 \\end{bmatrix}\\)Here, \\(A = -A^H\\).</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#18-idempotent-matrix","title":"18. Idempotent Matrix","text":"<p>A square matrix A is idempotent if: \\(A^2 = A\\) or \\(A^n\u00a0= A\\), for every \\(n \u2265 2\\)</p> <p>This means multiplying the matrix (not element-wise) by itself returns the same matrix.</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#19-nilpotent-matrix","title":"19. Nilpotent Matrix","text":"<p>A square matrix \\(A\\) of order \\(n\\) is nilpotent if there exists an integer \\(k \\leq n\\) such that: \\(A^k = 0\\)where \\(0\\) is the zero matrix.</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#20-involutory-matrix","title":"20. Involutory Matrix","text":"<p>A square matrix \\(A\\) is involutory if it is its own inverse: \\(A^{-1} = A\\)</p> <p>For example, an identity matrix is involutory as it is equal to its\u00a0inverse.</p>"},{"location":"linear-algebra/introduction-to-vectors/matrices/#resources","title":"Resources","text":"<ol> <li>https://math.mit.edu/~dyatlov/54summer10/matalg.pdf</li> </ol>"},{"location":"linear-algebra/introduction-to-vectors/planes/","title":"Planes","text":""},{"location":"linear-algebra/introduction-to-vectors/planes/#what-is-a-plane","title":"What is a Plane?","text":"<p>A plane is a flat, two-dimensional surface that extends infinitely in 3D space. Think of it like an endless sheet of paper floating in space, it has length and width, but no thickness.</p>"},{"location":"linear-algebra/introduction-to-vectors/planes/#how-do-we-define-a-plane","title":"How Do We Define a Plane?","text":"<p>A plane in 3D space can be defined in two main ways:</p>"},{"location":"linear-algebra/introduction-to-vectors/planes/#1-using-a-point-and-a-normal-vector-general","title":"1. Using a Point and a Normal Vector (General)","text":"<p>The general equation of a plane is:</p> \\[ \\begin{gathered} ax+by+cz=d \\\\ \\vec{n}\\cdot(\\vec{r}-\\vec{r_0}) \\end{gathered} \\] <p>Here:</p> <ul> <li>\\(\\vec{n}=\u27e8a,b,c\u27e9\\) is the normal vector i.e. a vector that is perpendicular to the plane and <code>a,b,c</code> are the components of the vectors.</li> <li>\\(\\vec{r} = \u27e8x,y,z\u27e9\\) is a variable point on the plane.</li> <li>\\(\\vec{r_0} = \u27e8x_0,y_0,z_0\u27e9\\) is a known point on the plane.</li> <li><code>d</code> is a constant that shifts the plane away from the origin along the direction of the normal vector.</li> </ul> <p>This form tells us: any point <code>(x, y, z)</code> that satisfies the equation lies on the plane.</p> <p>Example:</p> \\[ \\begin{gathered} \\vec{n}\\cdot(\\vec{r}-\\vec{r_0}) = \\langle 4, 5, 6 \\rangle \\cdot \\langle x - 1, y - 2, z - 3 \\rangle \\\\ 4(x - 1) + 5(y - 2) + 6(z - 3) = 0 \\\\ 4x + 5y + 6z = 32 \\end{gathered} \\] <p>This describes a plane in 3D space with normal vector <code>(2,3,5)</code>.</p>"},{"location":"linear-algebra/introduction-to-vectors/planes/#2-using-two-vectors-parametric-form","title":"2. Using Two Vectors (Parametric Form)","text":"<p>A plane can also be defined using two linearly independent vectors that lie in the plane. Generally used for visualizations, sampling etc. If \\(v\\) and \\(w\\) are such vectors, then all points on the plane can be described by:</p> <p>To define a plane using vectors, you need:</p> <ul> <li>A point \\(\\mathbf{p} = (x_0, y_0, z_0)\\) that lies on the plane</li> <li>Two linearly independent vectors \\(v, w\\) that lie in the plane</li> </ul> <p>Then the parametric equation of the plane is:</p> \\[ r(s,t)=p+sv+tw \\] <p>Where:</p> <ul> <li>\\(s\\) and \\(t\\) are real numbers (parameters)</li> <li>\\(r(s,t)\\) gives all points on the plane</li> </ul>"},{"location":"linear-algebra/introduction-to-vectors/planes/#example","title":"Example:","text":"<p>Let:</p> <ul> <li>Point on the plane: \\(p=(1,0,0)\\)</li> <li>Direction vectors: \\(v=(2,2,0),  w=(0,2,1)\\)</li> </ul> <p>Then the plane is:</p> \\[ r(s,t)=(1,0,0)+s(2,2,0)+t(0,2,1) \\] <p>Expanding:</p> \\[ r(s,t)=(1+2s,2s+2t,t) \\] <p>This gives all points on the plane by varying \\(s\\) and \\(t\\).</p>"},{"location":"linear-algebra/introduction-to-vectors/planes/#why-are-planes-important","title":"Why Are Planes Important?","text":"<p>Planes are fundamental in geometry, linear algebra, computer graphics, and physics. They help:</p> <ul> <li>Visualize and solve systems of linear equations</li> <li>Define surfaces, collisions, or boundaries</li> <li>Represent flat objects like walls, screens, or layers in 3D models</li> </ul>"},{"location":"linear-algebra/introduction-to-vectors/planes/#resources","title":"Resources","text":"<ol> <li>https://www.youtube.com/watch?v=HjJ140TYbXQ</li> </ol>"},{"location":"linear-algebra/introduction-to-vectors/preface/","title":"Introduction to Vectors","text":""},{"location":"linear-algebra/introduction-to-vectors/preface/#what","title":"What","text":"<p>A vector is a mathematical object that has both magnitude (size) and direction. It is often represented as an arrow in space or as an ordered list of numbers.</p> <p>Example: In 2D, the vector v = [3, 4] represents a movement 3 units along the x-axis and 4 units along the y-axis.</p>"},{"location":"linear-algebra/introduction-to-vectors/preface/#how","title":"How","text":"<p>Vectors are typically written as coordinate pairs or triplets (like [x, y] or [x, y, z]) depending on the number of dimensions. You can perform operations such as:</p> <ul> <li>Addition: [1, 2] + [3, 4] = [4, 6]</li> <li>Scalar multiplication: 2 \u00d7 [3, 4] = [6, 8]</li> <li>Dot product: [1, 2] \u2022 [3, 4] = 1\u00d73 + 2\u00d74 = 11</li> </ul> <p>These operations are useful for combining or comparing vectors.</p>"},{"location":"linear-algebra/introduction-to-vectors/preface/#why","title":"Why","text":"<p>Vectors are fundamental in fields like physics, computer graphics, and machine learning because they model quantities with direction and magnitude. For instance:</p> <ul> <li>In physics, a force is a vector.</li> <li>In computer graphics, vectors describe movement and lighting.</li> <li>In machine learning, data points and weights are represented as vectors for efficient computation.</li> </ul>"},{"location":"linear-algebra/introduction-to-vectors/preface/#sections","title":"Sections","text":"<p>Vectors and Linear Combinations</p> <p>Planes</p> <p>Lengths and Dot Products</p> <p>Matrices</p>"},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/","title":"Vectors and Linear Combinations","text":""},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#what-is-a-linear-combination","title":"What is a Linear Combination?","text":"<p>A linear combination of two vectors means multiplying them by numbers (called scalars) and adding the results. For example:</p> \\[ 3v+5w \\] <p>This is a typical linear combination of the vectors v and w.</p> <p>If</p> \\[ \\mathbf{v} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\] <p>then</p> \\[ 3\\mathbf{v} + 5\\mathbf{w} = 3\\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} + 5\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 5 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 11 \\end{bmatrix} \\] <p>This means we move 3 units across (in x direction) and 11 units up (in y direction).</p>"},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#visualizing-vectors","title":"Visualizing Vectors","text":"<p>A vector like \\(\\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\)goes 2 steps in x and 3 steps in y. It points to the location (2, 3) in the xy-plane.</p> <p>All combinations like \\(\\mathbf{v} + d\\mathbf{w}\\) (fill in every point in the plane) if v and w point in different directions. They cover the whole 2D space.</p> <p>In 3D space, a combination like </p> \\[ c\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} + d\\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} \\] <p>fills a plane in xyz space (but not the full space unless we add a third independent vector).</p>"},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#vector-operations","title":"Vector Operations","text":"<p>A vector has multiple components, like:</p> \\[ \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} \\] <p>We treat v as a single object, not two separate numbers.</p>"},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#vector-addition","title":"Vector Addition","text":"<p>Add corresponding parts:</p> \\[ \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}, \\quad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\] <p>Then:</p> \\[ \\mathbf{v} + \\mathbf{w} = \\begin{bmatrix} v_1 + w_1 \\\\ v_2 + w_2 \\end{bmatrix} \\]"},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#vector-subtraction","title":"Vector Subtraction","text":"<p>Same idea:</p> \\[ \\mathbf{v} - \\mathbf{w} = \\begin{bmatrix} v_1 - w_1 \\\\ v_2 - w_2 \\end{bmatrix} \\]"},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>Multiply every part of a vector by a number:</p> \\[ \\mathbf{v} = 2\\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 2v_1 \\\\ 2v_2 \\end{bmatrix}, \\quad -\\mathbf{v} = \\begin{bmatrix} -v_1 \\\\ -v_2 \\end{bmatrix} \\] <p>A scalar is just a number like 2 or -1.</p> <p>The sum of a vector and its negative gives the zero vector:</p> \\[ \\mathbf{v} + (-\\mathbf{v}) = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\]  \ud83d\udca1 This is different from the number zero! It is a vector with all components equal to zero."},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#linear-combinations-in-practice","title":"Linear Combinations in Practice","text":"<p>A linear combination means: \\(c\\mathbf{v} + d\\mathbf{w}\\)</p> <p>You can also get:</p> <ul> <li>Sum: \\(1v+1w\\)</li> <li>Difference: \\(1v\u22121w\\)</li> <li>Zero vector: \\(0v+0w\\)</li> <li>Vector \\(cv\\) in the direction of \\(v\\): \\(cv+0wc\\)</li> </ul> <p>All of these are valid linear combinations.</p>"},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#representing-vectors","title":"Representing Vectors","text":"<p>There are three main ways to describe a vector v:</p> <ul> <li>By its components: (4, 2)</li> <li>As an arrow from (0, 0) to (4, 2)</li> <li>As a point at (4, 2) in the plane</li> </ul> <p>You can visualize v + w as going along v, then along w, or directly along the diagonal from start to finish.</p>"},{"location":"linear-algebra/introduction-to-vectors/vectors-and-linear-combinations/#vectors-in-3d","title":"Vectors in 3D","text":"<p>A 3D vector looks like: \\(\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{bmatrix}\\)This points from \\((0, 0, 0)\\) to \\((v\u2081, v\u2082, v\u2083)\\) in 3D space.</p> <p>Writing \\(\\mathbf{v} = (1, 1, -1)\\) is a shortcut for the column vector\\(\\begin{bmatrix} 1 \\\\ 1 \\\\ -1 \\end{bmatrix}\\)It is not a row vector which is the transpose.</p> <p>Addition in 3D works the same way: \\(\\mathbf{v} + \\mathbf{w} = \\begin{bmatrix} v_1 + w_1 \\\\ v_2 + w_2 \\\\ v_3 + w_3 \\end{bmatrix}\\)</p>"},{"location":"ml/about/","title":"About","text":"<p>Hi!</p>"},{"location":"nlp/about/","title":"About","text":"<p>Hi!</p>"},{"location":"rl/about/","title":"About","text":"<p>Hi!</p>"}]}